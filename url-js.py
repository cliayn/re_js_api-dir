import os
import re
import requests
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse
import hashlib

# ================== é€šç”¨å·¥å…·å‡½æ•° ==================
def get_valid_filename(url):
    """ä»URLç”Ÿæˆæœ‰æ•ˆçš„æ–‡ä»¶å"""
    parsed = urlparse(url)
    name = parsed.path.split("/")[-1] or "index"
    if not name.endswith('.js'):
        name += '.js'
    name = ''.join(c for c in name if c.isalnum() or c in ['-', '_', '.'])
    return name

def save_js_file(url, directory, session, referer):
    """ä¸‹è½½å¹¶ä¿å­˜JSæ–‡ä»¶"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/javascript, application/javascript, */*; q=0.01',
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': referer
        }
        
        response = session.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            filename = get_valid_filename(url)
            filepath = os.path.join(directory, filename)
            
            # å¤„ç†é‡å¤æ–‡ä»¶å
            counter = 1
            while os.path.exists(filepath):
                name, ext = os.path.splitext(filename)
                filepath = os.path.join(directory, f"{name}_{counter}{ext}")
                counter += 1
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            print(f"âœ… ä¿å­˜æˆåŠŸ: {os.path.basename(filepath)}")
            return filepath
        else:
            print(f"âŒ å“åº”çŠ¶æ€ç  {response.status_code}: {url}")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url}: {str(e)}")
    return None

def extract_js_links(url, session):
    """ä»URLä¸­æå–æ‰€æœ‰JSé“¾æ¥"""
    try:
        print(f"ğŸŒ æ­£åœ¨è§£æé¡µé¢: {url}")
        response = session.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        js_links = []
        
        for tag in soup.find_all(['script', 'link']):
            src = None
            if tag.name == 'script' and tag.get('src'):
                src = tag['src']
            elif tag.name == 'link' and tag.get('rel') == ['stylesheet'] and tag.get('href'):
                if tag['href'].endswith('.js'):
                    src = tag['href']
            
            if src:
                full_url = urljoin(url, src)
                if full_url.endswith('.js') or 'javascript' in tag.get('type', '').lower():
                    js_links.append(full_url)
        
        print(f"ğŸ” æ‰¾åˆ° {len(js_links)} ä¸ªJSæ–‡ä»¶é“¾æ¥")
        return js_links
    except Exception as e:
        print(f"âš ï¸ è§£æå¤±è´¥ {url}: {str(e)}")
        return []

def get_path_hash(paths):
    """è®¡ç®—è·¯å¾„é›†åˆçš„å“ˆå¸Œå€¼ç”¨äºæ¯”è¾ƒ"""
    sorted_paths = sorted(paths)
    path_str = ''.join(sorted_paths)
    return hashlib.md5(path_str.encode('utf-8')).hexdigest()

# ================== ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸‹è½½åŸå§‹JSæ–‡ä»¶ ==================
def download_initial_js_files(base_url, api_js_directory):
    """ä¸‹è½½ç›®æ ‡é¡µé¢æ‰€æœ‰JSæ–‡ä»¶"""
    os.makedirs(api_js_directory, exist_ok=True)
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive'
    })
    
    downloaded_files = []
    
    try:
        # è·å–ä¸»é¡µé¢JSé“¾æ¥
        js_links = extract_js_links(base_url, session)
        
        # ä¸‹è½½JSæ–‡ä»¶
        success_count = 0
        for i, js_url in enumerate(set(js_links), 1):
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ ({i}/{len(js_links)}): {js_url}")
            saved_path = save_js_file(js_url, api_js_directory, session, base_url)
            if saved_path:
                success_count += 1
                downloaded_files.append(saved_path)
        
        print(f"\nğŸ‰ åŸå§‹JSæ–‡ä»¶ä¸‹è½½å®Œæˆ! æˆåŠŸä¿å­˜ {success_count}/{len(js_links)} ä¸ªæ–‡ä»¶")
        return downloaded_files, session

    except Exception as e:
        print(f"âš ï¸ å‘ç”Ÿé”™è¯¯: {str(e)}")
        return downloaded_files, None

# ================== ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ†æJSæ–‡ä»¶æå–APIè·¯å¾„ ==================
def analyze_js_files_for_paths(api_js_directory):
    """åˆ†æJSæ–‡ä»¶å¹¶æå–APIè·¯å¾„"""
    pattern_groups = {
        "PagePath Matches": [
            (re.compile(r'pagePath:\s*"(.*?)"'), "pagePath_double_quotes"),
            (re.compile(r"pagePath:\s*'(.*?)'"), "pagePath_single_quotes"),
        ],
        "Path Matches": [
            (re.compile(r'path:\s*"(.*?)"'), "path_double_quotes"),
            (re.compile(r"path:\s*'(.*?)'"), "path_single_quotes"),
            (re.compile(r'url:\s*"([^"]+)"'), "url_double_quotes"),
            (re.compile(r"url:\s*'([^']+)'"), "url_single_quotes"),
        ]
    }

    # æ”¶é›†æ‰€æœ‰è·¯å¾„
    all_paths = set()
    
    # éå†JSæ–‡ä»¶
    total_files = 0
    processed_files = 0
    for root, _, files in os.walk(api_js_directory):
        for file in files:
            if file.endswith(".js"):
                total_files += 1
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        processed_files += 1
                        
                        # å¯¹æ¯ä¸ªåˆ†ç»„å’Œæ­£åˆ™è¡¨è¾¾å¼è¿›è¡ŒåŒ¹é…
                        for group, patterns in pattern_groups.items():
                            for pattern, _ in patterns:
                                matches = pattern.findall(content)
                                for match in matches:
                                    # æ ‡å‡†åŒ–è·¯å¾„ï¼šç¡®ä¿ä»¥/å¼€å¤´ï¼Œå»æ‰ç»“å°¾çš„/
                                    path = match.strip()
                                    if not path.startswith('/'):
                                        path = '/' + path
                                    if path.endswith('/'):
                                        path = path[:-1]
                                    all_paths.add(path)
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")

    print(f"\nğŸ” è·¯å¾„åˆ†æå®Œæˆ! å…±å¤„ç† {processed_files}/{total_files} ä¸ªJSæ–‡ä»¶")
    print(f"ğŸ“Š å‘ç° {len(all_paths)} ä¸ªå”¯ä¸€è·¯å¾„")
    
    return sorted(all_paths)

# ================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ„é€ å¹¶è¯·æ±‚æ–°URL ==================
def construct_and_request_urls(base_url, paths, session, api_js_directory):
    """æ„é€ ä¸¤ç§URLæ ¼å¼å¹¶è¯·æ±‚æ–°JSæ–‡ä»¶"""
    # è§£æåŸºç¡€URL
    parsed_base = urlparse(base_url)
    base_domain = f"{parsed_base.scheme}://{parsed_base.netloc}"
    
    # æ„é€ ä¸¤ç§URL
    direct_urls = []
    hash_urls = []
    
    for path in paths:
        # ç›´æ¥è¿æ¥æ ¼å¼: https://domain.com/path
        direct_url = urljoin(base_domain, path)
        direct_urls.append(direct_url)
        
        # #è¿æ¥æ ¼å¼: https://domain.com/#/path
        hash_path = f"/#{path}" if not path.startswith("#") else path
        hash_url = urlunparse((
            parsed_base.scheme,
            parsed_base.netloc,
            parsed_base.path,
            parsed_base.params,
            parsed_base.query,
            hash_path
        ))
        hash_urls.append(hash_url)
    
    print(f"\nğŸ”— æ„é€ äº† {len(direct_urls)} ä¸ªç›´æ¥è¿æ¥URL")
    print(f"ğŸ”— æ„é€ äº† {len(hash_urls)} ä¸ª#è¿æ¥URL")
    
    # ç”¨äºå­˜å‚¨æ‰€æœ‰æ–°å‘ç°çš„JSé“¾æ¥
    all_new_js_links = set()
    
    # è¯·æ±‚æ‰€æœ‰ç›´æ¥è¿æ¥URL
    print("\nğŸŒ å¼€å§‹è¯·æ±‚ç›´æ¥è¿æ¥URL...")
    for i, url in enumerate(direct_urls, 1):
        print(f"ğŸ” æ­£åœ¨å¤„ç†ç›´æ¥è¿æ¥ ({i}/{len(direct_urls)}): {url}")
        try:
            js_links = extract_js_links(url, session)
            all_new_js_links.update(js_links)
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        except Exception as e:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ {url}: {str(e)}")
    
    # è¯·æ±‚æ‰€æœ‰#è¿æ¥URL
    print("\nğŸŒ å¼€å§‹è¯·æ±‚#è¿æ¥URL...")
    for i, url in enumerate(hash_urls, 1):
        print(f"ğŸ” æ­£åœ¨å¤„ç†#è¿æ¥ ({i}/{len(hash_urls)}): {url}")
        try:
            js_links = extract_js_links(url, session)
            all_new_js_links.update(js_links)
            time.sleep(0.5)
        except Exception as e:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ {url}: {str(e)}")
    
    print(f"\nğŸ” æ€»å…±å‘ç° {len(all_new_js_links)} ä¸ªæ–°çš„JSæ–‡ä»¶é“¾æ¥")
    
    # ä¸‹è½½æ–°å‘ç°çš„JSæ–‡ä»¶
    success_count = 0
    new_files = []
    for i, js_url in enumerate(all_new_js_links, 1):
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–°JSæ–‡ä»¶ ({i}/{len(all_new_js_links)}): {js_url}")
        saved_path = save_js_file(js_url, api_js_directory, session, base_url)
        if saved_path:
            success_count += 1
            new_files.append(saved_path)
    
    print(f"\nğŸ‰ æ–°JSæ–‡ä»¶ä¸‹è½½å®Œæˆ! æˆåŠŸä¿å­˜ {success_count}/{len(all_new_js_links)} ä¸ªæ–‡ä»¶")
    return new_files

# ================== ç¬¬å››éƒ¨åˆ†ï¼šæœ€ç»ˆè·¯å¾„åˆ†æè¾“å‡º ==================
def final_path_analysis(api_js_directory, output_file):
    """å¯¹æ‰€æœ‰JSæ–‡ä»¶è¿›è¡Œæœ€ç»ˆè·¯å¾„åˆ†æ"""
    # å®šä¹‰åŒ¹é…çš„æ­£åˆ™è¡¨è¾¾å¼å’Œåˆ†ç±»æ ‡ç­¾
    pattern_groups = {
        "PagePath Matches": [
            (re.compile(r'pagePath:\s*"(.*?)"'), "pagePath_double_quotes"),
            (re.compile(r"pagePath:\s*'(.*?)'"), "pagePath_single_quotes"),
        ],
        "Path Matches": [
            (re.compile(r'path:\s*"(.*?)"'), "path_double_quotes"),
            (re.compile(r"path:\s*'(.*?)'"), "path_single_quotes"),
            (re.compile(r'url:\s*"([^"]+)"'), "url_double_quotes"),
            (re.compile(r'url: "([^"]+)'), "url_double_quotes"),
        ],
        "GET Matches": [
            (re.compile(r'get\([^()]*?"([^"]*?)"[^()]*?\)'), "get_double_quotes"),
            (re.compile(r"get\([^()]*?['\"]([^'\"]*?)['\"][^()]*?\)"), "get_single_quotes"),
        ],
        "POST Matches": [
            (re.compile(r'post\([^()]*?"([^"]*?)"[^()]*?\)'), "post_double_quotes"),
            (re.compile(r"POST\([^()]*?['\"]([^'\"]*?)['\"][^()]*?\)"), "post_single_quotes"),
        ]
    }

    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    matched_results = {group: [] for group in pattern_groups}
    total_files = 0
    processed_files = 0

    # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰ .js æ–‡ä»¶
    for root, _, files in os.walk(api_js_directory):
        for file in files:
            if file.endswith(".js"):
                total_files += 1
                file_path = os.path.join(root, file)
                try:
                    # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å–å†…å®¹
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        processed_files += 1
                        
                        # å¯¹æ¯ä¸ªåˆ†ç»„å’Œæ­£åˆ™è¡¨è¾¾å¼è¿›è¡ŒåŒ¹é…
                        for group, patterns in pattern_groups.items():
                            for pattern, label in patterns:
                                matches = pattern.findall(content)
                                if matches:
                                    for match in matches:
                                        # æŒ‰åˆ†ç»„ä¿å­˜åŒ¹é…çš„è·¯å¾„å’Œæ ‡ç­¾
                                        matched_results[group].append((match.strip(), label))
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")

    print(f"\nğŸ” æœ€ç»ˆè·¯å¾„åˆ†æå®Œæˆ! å…±å¤„ç† {processed_files}/{total_files} ä¸ªJSæ–‡ä»¶")
    
    # å°†ç»“æœå†™å…¥æ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            total_paths = 0
            
            for group, results in matched_results.items():
                if results:
                    f.write(f"===== {group} =====\n")
                    # å»é‡ã€æ’åº
                    unique_results = sorted(set(results), key=lambda x: x[0])
                    max_length = max(len(path) for path, _ in unique_results) if unique_results else 0
                    
                    for path, label in unique_results:
                        f.write(f"{path.ljust(max_length + 2)}\t[{label}]\n")
                        total_paths += 1
                    f.write("\n")  # åˆ†å—ä¹‹é—´åŠ ç©ºè¡Œ
            
            f.write(f"===== ç»Ÿè®¡ä¿¡æ¯ =====\n")
            f.write(f"æ€»æå–è·¯å¾„æ•°: {total_paths}\n")
            f.write(f"åˆ†ææ–‡ä»¶æ•°: {processed_files}\n")
            f.write(f"æ–‡ä»¶æ¥æºç›®å½•: {api_js_directory}\n")
            f.write(f"åˆ†ææ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            
        print(f"ğŸ“ åŒ¹é…çš„è·¯å¾„å·²æŒ‰åˆ†ç±»å†™å…¥ {output_file}")
        print(f"ğŸ“Š å…±æå– {total_paths} ä¸ªAPIè·¯å¾„")
        return total_paths
    except Exception as e:
        print(f"âš ï¸ æ— æ³•å†™å…¥æ–‡ä»¶ {output_file}: {e}")
        return 0

# ================== ä¸»ç¨‹åº ==================
if __name__ == "__main__":
    # é…ç½®
    api_js_directory = r"D:\Desktop\æ‚é¡¹\ç¼´è´¹"
    output_file = os.path.join(api_js_directory, "path.txt")
    
    # ç”¨æˆ·è¾“å…¥URL
    target_url = input("è¯·è¾“å…¥ç›®æ ‡URL: ").strip()
    
    # ç¬¬ä¸€æ­¥ï¼šä¸‹è½½åŸå§‹JSæ–‡ä»¶
    print("\n" + "="*60)
    print("ç¬¬ä¸€æ­¥ï¼šä¸‹è½½åŸå§‹JSæ–‡ä»¶")
    print("="*60)
    initial_files, session = download_initial_js_files(target_url, api_js_directory)
    
    if session is None:
        print("\nâŒ åˆå§‹ä¸‹è½½å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        exit(1)
    
    # ç¬¬äºŒæ­¥ï¼šåˆå§‹è·¯å¾„åˆ†æ
    print("\n\n" + "="*60)
    print("ç¬¬äºŒæ­¥ï¼šåˆå§‹è·¯å¾„åˆ†æ")
    print("="*60)
    paths = analyze_js_files_for_paths(api_js_directory)
    
    # å¾ªç¯æ§åˆ¶å˜é‡
    max_iterations = 5  # æœ€å¤§å¾ªç¯æ¬¡æ•°
    iteration = 1
    previous_paths = set()
    current_paths = set(paths)
    all_new_files = []
    
    # è·¯å¾„å‘ç°å¾ªç¯
    while iteration <= max_iterations and previous_paths != current_paths:
        print(f"\n\n" + "="*60)
        print(f"è·¯å¾„å‘ç°å¾ªç¯ #{iteration}/{max_iterations}")
        print("="*60)
        
        # ä¿å­˜å½“å‰è·¯å¾„ç”¨äºä¸‹æ¬¡æ¯”è¾ƒ
        previous_paths = current_paths
        
        if paths:
            # æ˜¾ç¤ºå‰5ä¸ªè·¯å¾„ä½œä¸ºç¤ºä¾‹
            print("\nğŸ“‹ éƒ¨åˆ†æå–è·¯å¾„ç¤ºä¾‹:")
            for i, path in enumerate(paths[:5], 1):
                print(f"{i}. {path}")
            if len(paths) > 5:
                print(f"... ä»¥åŠå¦å¤– {len(paths)-5} ä¸ªè·¯å¾„")
            
            # ç¬¬ä¸‰æ­¥ï¼šæ„é€ URLå¹¶è¯·æ±‚æ–°JSæ–‡ä»¶
            print("\n" + "-"*50)
            print("æ„é€ URLå¹¶è¯·æ±‚æ–°JSæ–‡ä»¶")
            print("-"*50)
            new_files = construct_and_request_urls(target_url, paths, session, api_js_directory)
            all_new_files.extend(new_files)
            
            # ç¬¬å››æ­¥ï¼šå†æ¬¡åˆ†æè·¯å¾„
            print("\n" + "-"*50)
            print("å†æ¬¡åˆ†æJSæ–‡ä»¶æå–è·¯å¾„")
            print("-"*50)
            new_paths = analyze_js_files_for_paths(api_js_directory)
            current_paths = set(new_paths)
            
            # è®¡ç®—æ–°å¢è·¯å¾„æ•°
            new_path_count = len(current_paths - previous_paths)
            print(f"ğŸ“ˆ æ–°å¢è·¯å¾„æ•°: {new_path_count}")
            
            # å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£
            paths = new_paths
            iteration += 1
        else:
            print("\nâš ï¸ æœªæå–åˆ°ä»»ä½•è·¯å¾„ï¼Œç»ˆæ­¢å¾ªç¯")
            break
    
    # æœ€ç»ˆè·¯å¾„åˆ†æ
    print("\n\n" + "="*60)
    print("æœ€ç»ˆè·¯å¾„åˆ†æè¾“å‡º")
    print("="*60)
    final_path_analysis(api_js_directory, output_file)
    
    # æœ€ç»ˆæŠ¥å‘Š
    print("\n\n" + "="*60)
    print("âœ… ä»»åŠ¡å®Œæˆ!")
    print("="*60)
    print(f"ğŸ“ ä¿å­˜ç›®å½•: {api_js_directory}")
    print(f"ğŸ“„ åˆå§‹ä¸‹è½½JSæ–‡ä»¶: {len(initial_files)} ä¸ª")
    print(f"ğŸ“„ æ–°å‘ç°JSæ–‡ä»¶: {len(all_new_files)} ä¸ª")
    print(f"ğŸ”„ å¾ªç¯æ¬¡æ•°: {iteration-1}/{max_iterations}")
    print(f"ğŸ“Š æœ€ç»ˆè·¯å¾„æ•°: {len(current_paths)}")
    print(f"ğŸ“ æœ€ç»ˆåˆ†æç»“æœ: {output_file}")